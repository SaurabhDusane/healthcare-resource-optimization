{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Data Acquisition Guide\n",
    "## Healthcare Resource Optimization Project\n",
    "\n",
    "This notebook guides you through acquiring the CDC NHAMCS dataset and setting up API credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CDC NHAMCS Dataset\n",
    "\n",
    "### Download Instructions:\n",
    "1. Visit: https://www.cdc.gov/nchs/ahcd/datasets_documentation_related.htm\n",
    "2. Download the Emergency Department (ED) data files\n",
    "3. Save to `data/raw/nhamcs/` directory\n",
    "\n",
    "### Required Files:\n",
    "- NHAMCS ED 2021 data file (CSV or SAS format)\n",
    "- Documentation file\n",
    "- Codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create directory structure\n",
    "directories = [\n",
    "    'data/raw/nhamcs',\n",
    "    'data/raw/cdc_news',\n",
    "    'data/raw/reddit_health',\n",
    "    'data/raw/twitter_health',\n",
    "    'data/processed',\n",
    "    'models',\n",
    "    'logs',\n",
    "    'visualizations'\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"Created: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reddit API Setup\n",
    "\n",
    "### Steps:\n",
    "1. Go to: https://www.reddit.com/prefs/apps\n",
    "2. Click \"Create App\" or \"Create Another App\"\n",
    "3. Fill in:\n",
    "   - Name: Healthcare Trend Analyzer\n",
    "   - Type: Script\n",
    "   - Redirect URI: http://localhost:8080\n",
    "4. Copy Client ID and Secret to `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Reddit API connection\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Reddit API Configuration:\")\n",
    "print(f\"Client ID configured: {'REDDIT_CLIENT_ID' in os.environ}\")\n",
    "print(f\"Client Secret configured: {'REDDIT_CLIENT_SECRET' in os.environ}\")\n",
    "print(f\"User Agent configured: {'REDDIT_USER_AGENT' in os.environ}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Installation\n",
    "\n",
    "Check all required packages are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "required_packages = [\n",
    "    'pandas', 'numpy', 'scipy', 'scikit-learn',\n",
    "    'requests', 'beautifulsoup4', 'praw',\n",
    "    'textblob', 'matplotlib', 'seaborn', 'plotly'\n",
    "]\n",
    "\n",
    "print(\"Package Installation Check:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"✓ {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"✗ {package} - NOT INSTALLED\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Next Steps\n",
    "\n",
    "After completing data acquisition:\n",
    "1. Run notebook 02: Web Scraping - CDC\n",
    "2. Run notebook 03: Web Scraping - Reddit\n",
    "3. Run notebook 04: Web Scraping - Twitter\n",
    "4. Proceed to data cleaning and analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
